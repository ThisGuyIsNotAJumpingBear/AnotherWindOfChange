{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import numpy as np\n",
    "from utils import load_instances, load_labels, load_annotator_labels\n",
    "import pickle \n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"], organization=\"org-6u1yKGMuXAyb3dStdjvmFHMo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since running the prompts through chatgpt are ran through multiprocessing, this `ipynb` will only run metrics on saved prompt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_files = [('gpt3.5', 'data/gpt3.5_answers_qa.pkl'), ('gpt4', 'data/gpt4_answers_qa.pkl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt3.5 accuracy: 0.5866747060596925\n",
      "gpt4 accuracy: 0.26\n"
     ]
    }
   ],
   "source": [
    "for model_name, file in response_files:\n",
    "    with open(file, 'rb') as fp:\n",
    "            chatgpt_answers = pickle.load(fp)\n",
    "        \n",
    "    count = 0\n",
    "    total_count = 0\n",
    "    for para_lst in chatgpt_answers:\n",
    "        for response in para_lst:\n",
    "            if response[\"response\"] != \"1\" and response[\"response\"] != \"0\":\n",
    "                print(\"error\")\n",
    "            elif response[\"response\"] == response[\"label\"]:\n",
    "                count += 1\n",
    "            \n",
    "            total_count += 1\n",
    "    print(f\"{model_name} accuracy: {count / total_count}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_to_target_word():\n",
    "    instances = load_instances()\n",
    "    \n",
    "    id_target_dict = {}\n",
    "    for instance in instances:\n",
    "        id_target_dict[instance[\"id\"]] = instance[\"word\"]\n",
    "    \n",
    "    return id_target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt3.5 pearson corr: -0.16726689817294357, p-value: 0.3443892890379424\n",
      "gpt4 pearson corr: 0.6866484682666557, p-value: 7.324716289807194e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ids_to_word = get_id_to_target_word()\n",
    "annotator_labels = load_annotator_labels()\n",
    "\n",
    "for model_name, file in response_files:\n",
    "\n",
    "    with open(file, 'rb') as fp:\n",
    "        chatgpt_answers = pickle.load(fp)\n",
    "\n",
    "    meaning_change = defaultdict(list)\n",
    "\n",
    "    for para_lst in chatgpt_answers:\n",
    "        for response in para_lst:\n",
    "            target_word = ids_to_word[response[\"id\"]]\n",
    "            if response[\"response\"] != \"1\" and response[\"response\"] != \"0\":\n",
    "                print(\"error\")\n",
    "            else:\n",
    "                meaning_change[target_word].append(int(response[\"response\"]))\n",
    "\n",
    "    mc_dict = {}\n",
    "    for key in list(meaning_change.keys()):\n",
    "        mc_dict[key] = np.mean(np.asarray(meaning_change[key]))\n",
    "\n",
    "    \n",
    "\n",
    "    gpt_vec = []\n",
    "    annotator_vec = []\n",
    "\n",
    "    for key in annotator_labels.keys():\n",
    "        gpt_vec.append(float(mc_dict[key]))\n",
    "        annotator_vec.append(float(annotator_labels[key]))\n",
    "\n",
    "\n",
    "    pearson, p_value = pearsonr(gpt_vec, annotator_vec)\n",
    "    print(f\"{model_name} pearson corr: {pearson}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt3.5 f1: 0.011535688536409516\n",
      "gpt4 f1: 0.38333333333333336\n"
     ]
    }
   ],
   "source": [
    "#f1 not a good metric as we have a lot more true negatives\n",
    "\n",
    "for model_name, file in response_files:\n",
    "\n",
    "    with open(file, 'rb') as fp:\n",
    "        chatgpt_answers = pickle.load(fp)\n",
    "\n",
    "    response_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for para_lst in chatgpt_answers:\n",
    "        for response in para_lst:\n",
    "            label_list.append(response[\"label\"])\n",
    "            response_list.append(response[\"response\"])\n",
    "\n",
    "    f1 = f1_score(label_list, response_list, average=\"binary\", pos_label=\"1\")\n",
    "    print(f\"{model_name} f1: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9a902e6a5cf10f0266cd56ad91f8b2e66b295b7f56b43a091f5e49920b976f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
