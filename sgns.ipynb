{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "from utils import get_sorted_tweets, get_target_words, load_annotator_labels\n",
    "import gensim\n",
    "from procrustes import orthogonal\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from utils import get_target_words\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: Word2Vec<vocab=1271, vector_size=100, alpha=0.025>\n",
      "2020: Word2Vec<vocab=8861, vector_size=100, alpha=0.025>\n",
      "2021: Word2Vec<vocab=7591, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#since dataset is small, we chose a larger k \n",
    "date_dict = get_sorted_tweets()\n",
    "\n",
    "years = [\"2019\", \"2020\", \"2021\"]\n",
    "for year in years:\n",
    "\n",
    "    k = 5\n",
    "    model = gensim.models.Word2Vec(\n",
    "        sg=1, # skipgram\n",
    "        hs=0, # negative sampling\n",
    "        negative=k, # number of negative samples\n",
    "        workers=4,\n",
    "        vector_size=100\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    # sentences = PathLineSentences(corpDir)\n",
    "    \n",
    "    sentence_list = []\n",
    "    data_year = date_dict[year]\n",
    "    for data in data_year:\n",
    "        sentence_list.append(data[\"tokens\"])\n",
    "\n",
    "    model.build_vocab(sentence_list)\n",
    "    model.train(sentence_list, total_examples=model.corpus_count, epochs=20)\n",
    "\n",
    "    print(f\"{year}: {model}\")\n",
    "\n",
    "    # # Save the vectors and the model\n",
    "    outpath = f'model_files/sngs_{year}'\n",
    "    model.wv.save(outpath)\n",
    "    model.save(outpath + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = get_target_words()\n",
    "    \n",
    "word2vec_path = \"model_files/sngs_2019\"\n",
    "word2vec = gensim.models.KeyedVectors.load(word2vec_path)\n",
    "w2v_vocabulary = word2vec.key_to_index\n",
    "\n",
    "words = list( w2v_vocabulary.keys())\n",
    "\n",
    "target_words = set()\n",
    "\n",
    "date_dict = get_sorted_tweets()\n",
    "\n",
    "for data in date_dict[year]:\n",
    "    target_words.add(data[\"word\"])\n",
    "\n",
    "with open(f'data/target_words_{year}.pkl', 'wb') as fp:\n",
    "    pickle.dump(list(target_words), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_dist(v1, v2):\n",
    "    return 1 - metrics.pairwise.cosine_similarity(v1.reshape(1, -1), v2.reshape(1, -1))[0][0]\n",
    "\n",
    "def get_same_words(word_set1, word_set2):\n",
    "    return word_set1.intersection(word_set2)\n",
    "\n",
    "def create_matrices(word2vec1, word2vec2):\n",
    "\n",
    "    vector_size = 100\n",
    "    vocab1 = set(word2vec1.index_to_key)\n",
    "    vocab2 = set(word2vec2.index_to_key)\n",
    "\n",
    "    intersect = get_same_words(vocab1, vocab2)\n",
    "\n",
    "    mat1 = np.zeros((len(intersect), vector_size))\n",
    "    mat2 = np.zeros((len(intersect), vector_size))\n",
    "\n",
    "    for i, word in enumerate(intersect):\n",
    "        mat1[i] = word2vec1.get_vector(word)\n",
    "        mat2[i] = word2vec2.get_vector(word)\n",
    "    \n",
    "\n",
    "    return list(intersect), mat1, mat2\n",
    "\n",
    "def get_consine_distance(year1, year2):\n",
    "    target_words = get_target_words()\n",
    "\n",
    "    word2vec_path1 = f\"model_files/sngs_{year1}\"\n",
    "    word2vec_path2 = f\"model_files/sngs_{year2}\"\n",
    "    word2vec1 = gensim.models.KeyedVectors.load(word2vec_path1)\n",
    "    word2vec2 = gensim.models.KeyedVectors.load(word2vec_path2)\n",
    "\n",
    "    intersect, mat1, mat2 = create_matrices(word2vec1, word2vec2)\n",
    "\n",
    "    result = orthogonal(mat1, mat2, scale=True, translate=True)\n",
    "    # display_procrutes_result(result)\n",
    "    a_op = np.dot(result.new_a, result.t)\n",
    "    cosine_distances = {}\n",
    "    for target in target_words:\n",
    "        try:\n",
    "            idx = intersect.index(target)\n",
    "            cosine_distances[target] = cosine_dist(a_op[idx], result.new_b[idx])\n",
    "        except:\n",
    "            pass\n",
    "            # print(f\"target word {target} is not found in the intersect of both corpora\")\n",
    "    \n",
    "    return cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson correlation to annotated labels\n",
      "pearson coeff and p-value for year 2019-2020: 0.3560518830300495, 0.13460947157852474\n",
      "pearson coeff and p-value for year 2020-2021: 0.06471005315346301, 0.8051081443290519\n"
     ]
    }
   ],
   "source": [
    "years = [(\"2019\", \"2020\"), (\"2020\", \"2021\")]\n",
    "\n",
    "print(\"pearson correlation to annotated labels\") \n",
    "\n",
    "for year1, year2 in years:\n",
    "    cd = get_consine_distance(year1, year2)\n",
    "    \n",
    "    labels = load_annotator_labels()\n",
    "\n",
    "    sgns_vec = []\n",
    "    annotator_vec = []\n",
    "\n",
    "    for key in cd.keys():\n",
    "        sgns_vec.append(float(cd[key]))\n",
    "        annotator_vec.append(float(labels[key]))\n",
    "\n",
    "    \n",
    "    pearson, p_value = pearsonr(sgns_vec, annotator_vec)\n",
    "    print(f\"pearson coeff and p-value for year {year1}-{year2}: {pearson}, {p_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9a902e6a5cf10f0266cd56ad91f8b2e66b295b7f56b43a091f5e49920b976f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
