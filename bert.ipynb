{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from utils import get_sorted_tweets, get_target_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name).cuda()\n",
    "tweets = get_sorted_tweets()\n",
    "target_words = get_target_words()\n",
    "tsv_file_path = 'data/annotator.tsv'\n",
    "# Read the TSV file into a pandas DataFrame\n",
    "df = pd.read_csv(tsv_file_path, sep=' ', header=None).to_numpy()\n",
    "annotator = {item[0]: item[1] for item in df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector_from_context(word, text):\n",
    "    tok_w = tokenizer(word, return_tensors='pt', add_special_tokens=False)\n",
    "    tok = int(tok_w['input_ids'].flatten()[0])\n",
    "    len_tok = len(tok_w['input_ids'].flatten())\n",
    "    tok_t = tokenizer(text, return_tensors='pt', padding='max_length')\n",
    "    ids = tok_t['input_ids'].flatten().tolist()\n",
    "    if tok in ids:\n",
    "        idx = ids.index(tok)\n",
    "    else:\n",
    "        raise ValueError(f'{tok} from {tok_w} not in list {ids}. \\n text: {text} word {word} \\n tokenizer decode: {tokenizer.decode(ids)}')\n",
    "    for item in tok_t:\n",
    "        tok_t[item] = tok_t[item].to('cuda')\n",
    "    vec = model(**tok_t)['last_hidden_state'].squeeze(0)[idx:idx+len_tok].cpu().detach().numpy()\n",
    "    vec = np.average(vec, axis=0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vector_by_year(year):\n",
    "    target_word_vectors = {wrd: [] for wrd in target_words}\n",
    "    data = tweets[year]\n",
    "    for t in data:\n",
    "        word = t['word']\n",
    "        text = t['text']\n",
    "        try:\n",
    "            vec = generate_vector_from_context(word, text)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        target_word_vectors[word].append(vec)\n",
    "    for wrd in target_words:\n",
    "        vecs = np.array(target_word_vectors[wrd])\n",
    "        target_word_vectors[wrd] = np.average(vecs, axis=0)\n",
    "    \n",
    "    return target_word_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ROG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "c:\\Users\\ROG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "vecs_2019 = avg_vector_by_year('2019')\n",
    "vecs_2020 = avg_vector_by_year('2020')\n",
    "vecs_2021 = avg_vector_by_year('2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan in vecs_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlation(A, B):\n",
    "    bert_pred = []\n",
    "    ground_truth = []\n",
    "    for key in A:\n",
    "        a = A[key]\n",
    "        b = B[key]\n",
    "        if type(a) != np.ndarray or type(b) != np.ndarray:\n",
    "            continue\n",
    "        cos_sim = float(cosine_similarity([a], [b]).flatten()[0])\n",
    "        cos_dist = 1 - cos_sim\n",
    "        bert_pred.append(cos_dist)\n",
    "        ground_truth.append(annotator[key])\n",
    "    return pearsonr(bert_pred, ground_truth)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012166736321230068"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_correlation(vecs_2020, vecs_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, labels_path):\n",
    "    # Load tweet instances\n",
    "    with open(data_path, 'r', encoding='utf-8') as file:\n",
    "        data_instances = [json.loads(line) for line in file]\n",
    "\n",
    "    # Load labels\n",
    "    with open(labels_path, 'r', encoding='utf-8') as file:\n",
    "        labels = dict(line.strip().split('\\t') for line in file)\n",
    "\n",
    "    return data_instances, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'data/train.data.jl'\n",
    "train_labels_path = 'data/train.labels.tsv'\n",
    "\n",
    "val_data_path = 'data/validation.data.jl'\n",
    "val_labels_path = 'data/validation.labels.tsv'\n",
    "\n",
    "test_data_path = 'data/trial.data.jl'\n",
    "test_labels_path = 'data/trial.gold.tsv'\n",
    "\n",
    "data_instances, labels = [], {}\n",
    "tr_data, tr_labels = load_data(train_data_path, train_labels_path)\n",
    "data_instances.extend(tr_data)\n",
    "labels = labels | tr_labels\n",
    "\n",
    "val_data, val_labels = load_data(val_data_path, val_labels_path)\n",
    "data_instances.extend(val_data)\n",
    "labels = labels | val_labels\n",
    "\n",
    "te_data, te_labels = load_data(test_data_path, test_labels_path)\n",
    "data_instances.extend(te_data)\n",
    "labels = labels | te_labels\n",
    "\n",
    "pairs = {item['id']: [item['tweet1']['text'], item['tweet2']['text'], item['word']] for item in data_instances}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_acc(threshold):\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for key in pairs:\n",
    "        label = labels[key]\n",
    "        t1, t2, word = pairs[key]\n",
    "        try:\n",
    "            vec1 = generate_vector_from_context(word, t1)\n",
    "            vec2 = generate_vector_from_context(word, t2)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        res = float(cosine_similarity([vec1], [vec2]).flatten()[0])\n",
    "        res = 1 if res > threshold else 0\n",
    "        if res == int(label):\n",
    "            correct += 1\n",
    "        count += 1\n",
    "\n",
    "    return correct / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4515778019586507\n",
      "0.4515778019586507\n",
      "0.4515778019586507\n",
      "0.4515778019586507\n",
      "0.45973884657236125\n",
      "0.47279651795429817\n",
      "0.5337323177366703\n",
      "0.6447225244831338\n",
      "0.6142546245919478\n",
      "0.5522306855277476\n"
     ]
    }
   ],
   "source": [
    "for threshold in range(10):\n",
    "    i = threshold / 10\n",
    "    print(find_acc(i))\n",
    "# best threshold 0.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
