{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from nltk import bigrams\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from utils import get_sorted_tweets, get_target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = get_sorted_tweets()\n",
    "target_words = get_target_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppmi_by_year(year, dim):\n",
    "    tweets = get_sorted_tweets()\n",
    "    target_words = get_target_words()\n",
    "    tweets = tweets[str(year)]\n",
    "    words = []\n",
    "    for tweet in tweets:\n",
    "        words.extend(tweet['tokens'])\n",
    "\n",
    "    cnt = Counter(words)\n",
    "    common = [word[0] for word in cnt.most_common(2000)]\n",
    "    w = list(set(common)) + target_words\n",
    "    bigram_pairs = list(bigrams(words))\n",
    "    bigram_freq = FreqDist(bigram_pairs)\n",
    "    cooccur_matrix = np.zeros((len(w), len(w)))\n",
    "    for i in range(len(w)):\n",
    "        for j in range(len(w)):\n",
    "            freq = bigram_freq[(w[i], w[j])]\n",
    "            cooccur_matrix[i, j] = freq\n",
    "    total_occurrences = np.sum(cooccur_matrix)\n",
    "    row_sums = np.sum(cooccur_matrix, axis=1)\n",
    "    col_sums = np.sum(cooccur_matrix, axis=0)\n",
    "\n",
    "    ppmi_matrix = np.zeros_like(cooccur_matrix)\n",
    "\n",
    "    for i in range(cooccur_matrix.shape[0]):\n",
    "        for j in range(cooccur_matrix.shape[1]):\n",
    "            p_x_y = cooccur_matrix[i, j] / total_occurrences\n",
    "            p_x = row_sums[i] / total_occurrences\n",
    "            p_y = col_sums[j] / total_occurrences\n",
    "            \n",
    "            pmi = np.log2(p_x_y / (p_x * p_y))\n",
    "            if pmi != pmi:\n",
    "                pmi = 0\n",
    "            ppmi_matrix[i, j] = max(pmi, 0)\n",
    "    target_word_seq = []\n",
    "    for word in target_words:\n",
    "        idx = w.index(word)\n",
    "        target_word_seq.append(ppmi_matrix[idx])\n",
    "    ppmi_matrix = np.vstack(target_word_seq)\n",
    "\n",
    "    U, S, Vt = np.linalg.svd(ppmi_matrix, full_matrices=False)\n",
    "\n",
    "    U_k = U[:, :dim]\n",
    "    S_k = np.diag(S[:dim])\n",
    "    Vt_k = Vt[:, :dim]\n",
    "\n",
    "    reduced_matrix = np.dot(U_k, np.dot(S_k, Vt_k))\n",
    "    return reduced_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_161844\\1624441018.py:31: RuntimeWarning: divide by zero encountered in log2\n",
      "  pmi = np.log2(p_x_y / (p_x * p_y))\n",
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_161844\\1624441018.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  pmi = np.log2(p_x_y / (p_x * p_y))\n"
     ]
    }
   ],
   "source": [
    "dim = 300\n",
    "ppmi_matrix_2019 = get_ppmi_by_year(2019, dim)\n",
    "ppmi_matrix_2020 = get_ppmi_by_year(2020, dim)\n",
    "ppmi_matrix_2021 = get_ppmi_by_year(2021, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1629.5569561023312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.81775752e-02, 7.16158728e-03, 6.51641695e-04, 5.26622201e-01,\n",
       "       1.58930229e-02, 1.27899313e-01, 5.09733447e-03, 1.07818521e-01,\n",
       "       2.11466537e-01, 3.33332605e-01, 3.55947800e-01, 2.50400672e-01,\n",
       "       3.66968757e-01, 5.75781943e-02, 5.22041046e-03, 3.86168108e-02,\n",
       "       1.06218583e-01, 2.69006332e-01, 1.84030191e-01, 1.44408046e-01,\n",
       "       1.53596237e-02, 9.31517437e-01, 8.43768682e-02, 3.96587708e-02,\n",
       "       1.46531971e-01, 2.18946140e-01, 8.37373143e-02, 7.25161563e-02,\n",
       "       2.70390156e-01, 2.85753212e-02, 4.80566792e-02, 1.12126165e-02,\n",
       "       9.57676258e-05, 1.14963159e-01])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_2019 = orthogonal_procrustes(ppmi_matrix_2019, ppmi_matrix_2020)\n",
    "ppmi_matrix_2019 = ppmi_matrix_2019 @ r_2019[0]\n",
    "sim = cosine_similarity(ppmi_matrix_2019, ppmi_matrix_2020)\n",
    "1 - np.diag(sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
